---
title: 漫谈时间
---



> 最近用到了雪花算法(Snowflake)  ，Snowflake会与unix时间戳打交道，本文从雪花算法的谈起，聊一聊平时容易忽略的Linux对于时间的一些处理。



## 从雪花算法说起

雪花算法（Snowflake）是Twitter在2010年开源的一个分布式ID生成算法[1]。主要解决了可以在分布式系统快速生成自增ID的问题。美团2017年开源的leaf系统，也是使用了类雪花算法作为分布式ID生成器[2]。



雪花算法生成的ID大致如下：

```
|0 1 2 3 4 5 6 7|0 1 2 3 4 5 6 7|0 1 2 3 4 5 6 7|0 1 2 3 4 5 6 7|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|0|                         Timestamp(41bit)                    |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                       | Machine ID(10bit) |   Seq ID(10bit)   |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

Note：

1) ID长度为64bit

2) 第一位固定为0

3) 后41位是每1ms变化一次的时间戳

4) 10bit的机器ID

5) 10bit的Seq ID



算法利用Timestamp做到递增，41bit的时间戳，间隔如果代表1ms的话，大概能用2^41 / 1000 / 3600 / 24 / 365 = 69+年。利用10bit的机器码，保证不同机器上产生的ID不会重复。利用10bit的Seq ID，保证在1ms的间隔期间，每台机器可以产生2^10个Seq ID，即每台机器每毫秒，可以产生1024不重复的ID。本文并不深入讨论雪花算法本身，具体可以参考[1] [2]。



## 一些异常情况

雪花算法可以高效的分布式产生ID，以下一些异常情况需要考虑：

1) 如果在1ms内单台请求超过了1024，怎么办？

2) ID是基于timestamp有序的前提，如果timestamp发生跳跃怎么办？



对于1)，简单的做法是sleep(1ms - 已花费时间)，到下一毫秒时，又会有新的1024个ID可用。(或者直接异常，不能处理这样高的burst请求)

对于2)，在时间发生跳跃(如闰秒)的情况下，timestamp可能会回退到过去时间，此时有可能会产生重复的ID。最简单的处理是，如果发生时间回退，则异常报错。稍微优化一下，可以从Machine ID中取比如2bit，作为发生时间回退的标志位，当发生一次时间回退，则递增1，2bit可以抗短时间内4次时间回退。



## 为什么会时间跳跃

对于2）发生时间跳跃的情况下，timestamp可能会回退到过去这是怎么回事？直观理解，就是Linux时间发生了step跳跃，从时间点A，跳跃到时间点B（可能在A之前，也可能在A之后）。但是什么情况下会造成跳跃呢？一般Linux上会运行NTP服务，在运行了NTP服务的情况下会发生时间跳跃么？在此之前，先理解一下计算机系统中时间的概念，以及NTP是如何处理时间的。



## 什么是时间

那么什么是时间呢？这是一个tough的哲学问题，以下是一些解释[3]:

> *"Time has been invented in the universe so that everything would not happen at once."*
>
> *“Nobody can tell what Time is. However everybody does know its effects. Time implies memory and this is specially strange to consider this, in the context of computer science. Reversely does memory implies time in some way?*”



但对于计算来说，需要一个明确的定义。UTC (*Universal Time Coordinated*）是一个官方的对于描述“现在时间”的定义。数值上UTC时间与GMT (*Greenwich Mean Time*)时间相同。 但UTC时间本身是一个绝对的概念，它并不和任何时区(timezone)有关系，地球被划分成24个时区，每个时区拥有自己本地时间（如北京时间是UTC+8)。



对于Linux来说，时间如何存储的呢？一个Linux系统有两个Clocks，一个是有独立供电，可以在计算机关机情况下继续工作的“Real Time Clock”(或者叫做 "RTC", "CMOS Clock" or "Hardware Clock")，另一个是“System Clock”（或者叫做"Kernel Clock" or "Software Clock"） 这个时钟通常利用基于中断机制的软件计数器。通常情况下，计算机是非常难有一个很精准的时钟的（出于成本考虑也没有必要），因此出现了NTP服务用于校准时间。



## NTP校准

NTP（*Network Time Protocol*）是一个从Reference时钟服务器，同步到本地时间。那默认情况下NTP是如何调整时间的呢？NTP本身计算出自身与时钟源之间的偏差算法比较复杂，本文暂且不谈NTP本身同步时间的算法(本身算法参见[4])，当NTP计算出需要调整的时间后**默认**情况下按照以下逻辑处理：

1) 如果偏差小于+/- 128ms， 则NTP会通过调整内核时钟的PPM(Part per Million)的方式，使时钟“调慢”或"调快"。 简单理解就是当一定偏差范围内，NTP会保证时间在不发生跳跃的情况下，通过“调快/慢”内核时钟的方式，和精准时间靠近。

2) 如果偏差 [+/- 128ms, 1000s), 则NTP会通过跳跃方式，快速调整时间，此时时间是不连续的，可能会发生跳跃。

3) 如果偏差> 1000s, 则NTP会杀死自己，什么都不做。（除非NTP配置了-g选项，具体-g选项可参照man文档） 



当然，如果应用对于时间跳跃非常敏感，NTPD也提供机制只要加"-x"选项，就可以将偏差变为+/-600s的范围内都不会发生跳跃，而是慢慢矫正时间。只不过NTP矫正时间速度是有限的，NTP可以在1s内至多矫正+/- 0.5ms的时间，换句话说，1s的偏差值，需要大约最快2000s的时间将其修正。实际环境中是否启用"-x"，取决于业务需求。详见Redhat的KB[5]。



##一些发生时间跳跃的特殊情况

理解了NTP矫正时间的基本方式后，梳理一下以下情况可能出现时间跳跃：

* NTP没有配置"-x"参数时，时间和NTP时钟源时间间隔大于128ms：
  * 发生闰秒（过去40年中发生过25次闰秒的时间调整，都是+1秒，时间发生回退1s, 闰秒的处理是非常复杂的过程，此处并没有展开讨论，详见[6] [7]）
  * 系统启动时，读取硬件时钟到系统时钟，偏差>128ms时
  * 人为修改时间，如运行date -s，ntpdate修改系统时间

- NTP配置了"-x", 但时间差异大于600s时



## NTP时间精准度

对于正常情况下，启用了NTP的服务运行起来后，时间同步的精准程度，很大程度上取决于服务器到NTP，一般如果使用内网的NTP服务器，Server的时间精度可以控制在1ms数量级。如果使用公网时间服务器，这意味着Server到公网时间服务器之间的链路延迟会达到几十到上百ms，时间精度仅能控制到10ms数量级。



## 虚拟机ClockSource

在虚拟机环境中，有了NTP时刻进行时钟矫正，但是时钟本身的准确性/性能对于云上虚拟化环境来说也非常重要。在云上选择何种ClockSource，对于性能影响也不小。 对于Xen，ClockSource最好使用“tsc”  [9]， 测试[8]表明，对于时间敏感/CPU heavy的应用，Xen虚拟化的情况下（AWS之前主要的虚拟化，AWS的VM），获取时间的性能差了70+%。 目前腾讯云和AWS中国，都采用了KVM的方式，KVM默认使用kvm-clock。 Kvm-clock不会有[8]中描述的问题，kvm-clock支持vDSO。

你可以通过以下命令简单查看可用的 clocksource以及目前使用的 cloudsource.例如：

 ```
# cat /sys/devices/system/clocksource/clocksource0/available_clocksource
kvm-clock tsc acpi_pm
# cat /sys/devices/system/clocksource/clocksource0/current_clocksource
kvm-clock
 ```



## 线上注意点

对于线上系统，对于时间相关，以下注意点：

* 每个DC配置本地时钟源，服务器配置per DC时钟源。
* NTP配置-x选项，平滑处理闰秒和时间跳跃。
* NTP相关监控指标： 
  * Delay ： Server与时钟源之间的时延，可作为metrics监控时间源的质量，在本地DC的话，一般不会超过10ms。
  * Offset  ：  本地时间与时间服务器之间的时延，当大于128ms时，时钟发生了偏移，触发报警。
  * jitter ： offset的抖动，时钟源/Server时间越稳定，此值越小，内网服务器的情况下这个值一般<10.
* 当时间发生跳跃时，/var/log/message可以监控相关日志。
* 选择合适的ClockSource，获取更高性能(如：Xen使用TSC)
* NTP trouble shooting常用命令[10]



## Key Take away

1. 绝大多数情况下时钟不会跳跃，加上"-x"就可以cover闰秒和600s内时间偏差。
2. 需要对NTP关键指标监控。
3. 云上ClockSource要合理选择，对CPU密集型服务会有一定影响。





##Refer：

[1] https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake.html

[2] https://tech.meituan.com/2017/04/21/mt-leaf.html

[3] http://www.ntp.org/ntpfaq/NTP-s-time.htm

[4] https://tools.ietf.org/html/rfc5905

[5] https://access.redhat.com/articles/1456843

[6] https://access.redhat.com/articles/15145

[7] http://www.ntp.org/ntpfaq/NTP-s-time.htm#Q-TIME-LEAP-SECOND

[8] https://blog.packagecloud.io/eng/2017/03/08/system-calls-are-much-slower-on-ec2/

[9] https://opensource.com/article/17/6/timekeeping-linux-vms

[10] https://access.redhat.com/solutions/64868
